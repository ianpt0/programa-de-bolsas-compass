{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Imprimindo as primeiras 5 linhas do dataframe\n",
    "df_nomes.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema(). Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Imprimindo o esquema\n",
    "df_nomes.printSchema()\n",
    "\n",
    "# Imprimindo as primeiras 10 linhas do dataframe\n",
    "df_nomes.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Imprimindo as primeiras 10 linhas do dataframe\n",
    "df_nomes.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória. Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Imprimindo as primeiras 10 linhas do dataframe\n",
    "df_nomes.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Imprimindo as primeiras 10 linhas do dataframe\n",
    "df_nomes.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Selecione as pessoas que nasceram neste século (AnoNascimento >= 2000)\n",
    "df_select = df_nomes.filter(df_nomes[\"AnoNascimento\"] >= 2000)\n",
    "\n",
    "# Imprimindo as primeiras 10 linhas do dataframe\n",
    "df_select.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Usando Spark SQL repita o processo da Pergunta 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Registre o DataFrame como uma tabela SQL temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Execute a consulta SQL para selecionar pessoas que nasceram neste século\n",
    "df_select_sql = spark.sql(\"SELECT * FROM pessoas WHERE AnoNascimento >= 2000\")\n",
    "\n",
    "# Imprima as primeiras 10 linhas do resultado\n",
    "df_select_sql.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Filtre as linhas onde o \"AnoNascimento\" está entre 1980 e 1994\n",
    "df_millennials = df_nomes.filter((df_nomes[\"AnoNascimento\"] >= 1980) & (df_nomes[\"AnoNascimento\"] <= 1994))\n",
    "\n",
    "# Conte o número de linhas no dataframe resultante\n",
    "millennials_count = df_millennials.count()\n",
    "\n",
    "print(f\"O número de pessoas da geração Millennials é: {millennials_count}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Repita o processo da Pergunta 8 utilizando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Registre o DataFrame como uma tabela SQL temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Execute a consulta SQL para contar pessoas da geração Millennials\n",
    "millennials_count_sql = spark.sql(\"SELECT COUNT(*) AS MillennialsCount FROM pessoas WHERE AnoNascimento BETWEEN 1980 AND 1994\")\n",
    "\n",
    "# Imprima o resultado\n",
    "millennials_count_sql.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 | Exercícios Apache Spark\n",
    "#### Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade Baby Boomers – nascidos entre 1944 e 1964; Geração X – nascidos entre 1965 e 1979;4 Millennials (Geração Y) – nascidos entre 1980 e 1994; Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, floor, when\n",
    "\n",
    "# Definindo a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Exercicio Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lendo o arquivo 'nomes_aleatorios.txt' e carregando-o em um dataframe\n",
    "df_nomes = spark.read.text('nomes_aleatorios.txt')\n",
    "\n",
    "# Renomeando a coluna\n",
    "df_nomes = df_nomes.withColumnRenamed(\"value\", \"Nomes\")\n",
    "\n",
    "# Adicionando a coluna \"Escolaridade\" com valores aleatórios \"Fundamental\", \"Medio\" ou \"Superior\"\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \n",
    "                               when(rand() < 1/3, \"Fundamental\")\n",
    "                               .when(rand() < 2/3, \"Medio\")\n",
    "                               .otherwise(\"Superior\"))\n",
    "\n",
    "# Adicionando a coluna \"AnoNascimento\" com um ano aleatório entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", floor(rand() * (2010 - 1945 + 1)) + 1945)\n",
    "\n",
    "# Registre o DataFrame como uma tabela SQL temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Execute a consulta SQL para contar pessoas de cada país por geração\n",
    "generation_count_sql = spark.sql(\"\"\"\n",
    "    SELECT Pais, \n",
    "    CASE\n",
    "        WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'\n",
    "        WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Geração X'\n",
    "        WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'\n",
    "        WHEN AnoNascimento BETWEEN 1995 AND 2010 THEN 'Geração Z'\n",
    "    END as Geracao,\n",
    "    COUNT(*) as Quantidade\n",
    "    FROM pessoas\n",
    "    GROUP BY Pais, Geracao\n",
    "    ORDER BY Pais ASC, Geracao ASC, Quantidade ASC\n",
    "\"\"\")\n",
    "\n",
    "# Imprima o resultado\n",
    "generation_count_sql.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
